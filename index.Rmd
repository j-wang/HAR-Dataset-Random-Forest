---
title: "Predicting Behavior with Wearable Devices"
author: "James Wang"
output: html_document
---

## Synopsis

I used a random forest to classify actions using wearable sensor data from various individuals in the [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har) (also known as the Human Activity Recognition / HAR dataset). As literature and Kaggle competitions have shown, this results in a highly accurate classification, with our out-of-bag (OOB) and cross-validation error rate suggesting that our test set error rate will be <1% (which is confirmed by an actual run against our test set where we see 20/20 classifications correct).

## Data Cleanup and Feature Selection

```{r}
# Loading required libraries
library(ggplot2)
library(data.table)
library(randomForest)
```

In order to preprocess the HAR dataset to something I can use in the random forest, I remove the statistical transformations of the raw sensor data (e.g. avg, stddev, var). These transformation would be redundant at best, with their variation ultimately captured by the raw data, and potentially problematic at worst -- many elements from the transformations have missing data, while all raw data seems intact. 

I also remove the new window, timestamps, and num_window. since none of these correspond with raw sensor data that I would want to be my predictors, given that they are largely arbitrary values I would expect to be unrelated to the activity being predicted.

Finally, I do no transformations on the raw sensor data -- not even consolidation within each "window" (which represent a contiguous activity). The reason is that my test set will be individual data points within an activity, not a continuous reading. If I were trying to predict continuous motions, I would want my model to somehow capture the autocorrelation of actions within each window.

```{r}
training <- as.data.table(read.csv("pml-training.csv"))
testing <- as.data.table(read.csv("pml-testing.csv"))

preProc <- function(set) {
  filter <- "^(avg|stddev|var|amplitude|max|min|kurtosis|skewness|raw|cvtd|new|num|X)"
  final <- set[, -grep(filter, names(set)), with=FALSE]
}

trSet <- preProc(training)
teSet <- preProc(testing)
```

## Train Model

Here, I train the random forest model on my training set.

```{r, cache=TRUE}
set.seed(1324)
modFit <- randomForest(classe ~ ., data=trSet, importance=TRUE)
```

While random forests are hard to interpret, we can see the importance of individual predictors through mean accuracy decrease (normalized impact on prediction accuracy if the particular variable were removed). As we can see below, the belt and dumbbell magnet data tend to be the most important predictors.

```{r}
imp <- importance(modFit, type=1)
impFrame <- data.frame(vars=row.names(imp), MeanDecreaseAccuracy=imp)

ggplot(impFrame, aes(reorder(vars, MeanDecreaseAccuracy), MeanDecreaseAccuracy)) + 
  geom_bar(stat="identity") +
  coord_flip() +
  ylab("Mean Decrease Accuracy (Variable Importance)") +
  xlab("Sensor Reading") +
  ggtitle("Importance of Variables in Random Forest Model Fit")
```

We can start to get a sense of the performance of our model through seeing the out-of-bag (OOB) errors -- the errors against data not used in the construction of the trees in our random forest. This is generally a good estimate for test set error (but we'll do cross-validation anyway below). The other errors are for predictions on each individual class using our random forest. This shows us that we would expect our error rate to be very low (in the <1% range).

```{r}
plot(modFit, main="Model Error Against Number of Trees")
legend("top", colnames(modFit$err.rate), col=1:4, cex =0.8, fill=1:6, horiz=TRUE)
```

## Cross-Validation

Finally, in order to get a sense of how our data will perform on our test set, we can do a cross-validation test. Here, I do k-fold (where k=3), with a decreasing number of predictors in order to chart out accuracy vs. complexity of my model.

```{r crossval, cache=TRUE}
# Cross-validation with 3 folds, to prevent this from taking eternity
set.seed(1324)
result <- rfcv(trSet[, -grep("^classe", names(trSet)), with=FALSE], 
               trSet[, classe], cv.fold=3)
```

As we can see below, we start leveling off in error rate at around 7 variables. However, given the nature of random forest, I'm not too worried about a huge amount of overfit. The ensemble of trees generated by the method "voting" to give us the end result will tend to temper the amount of variance we'd expect -- in a way, "regularizing" our data for us. This corresponds with our OOB error rate, suggesting that our test set will have <1% error. (And from actually submitting against the test set and having 20/20 correct, this turns out to be true).

```{r}
with(result, plot(n.var, error.cv, log="x", 
                  type="o", lwd=2, 
                  xlab="Number of Variables",
                  ylab="Cross-Val Error Rate"))
title("k-fold Cross Validation (k=3)")
```
